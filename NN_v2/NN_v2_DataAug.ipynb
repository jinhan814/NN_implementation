{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "NN_v2_DataAug.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utCBcPcIebjX",
    "outputId": "c10e6ed0-d396-4af1-ef78-d90ecd192056"
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def __Conv(data, n=10):\n",
    "    x = data[1:].astype(np.float32) / 255\n",
    "    y = np.array([int(i == data[0]) for i in range(n)]).astype(np.float32)\n",
    "    return (x, y)\n",
    "\n",
    "def Conv(data):\n",
    "    ret = [*data]\n",
    "    for i in range(len(ret)):\n",
    "        ret[i] = __Conv(ret[i])\n",
    "    return ret\n",
    "\n",
    "def load_data():\n",
    "    mnist = keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.reshape(-1, 784)\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    x_test = x_test.reshape(-1, 784)\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "    train_data = np.concatenate((y_train, x_train), axis = 1)\n",
    "    test_data = np.concatenate((y_test, x_test), axis = 1)\n",
    "    train_data = Conv(train_data)\n",
    "    test_data  = Conv(test_data)\n",
    "\n",
    "    return (train_data, test_data)\n",
    "\n",
    "train_data, test_data = load_data()\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Eu1Jb5jpumYj"
   },
   "source": [
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def deriv(self, x):\n",
    "        return self(x) * (1 - self(x))\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def __call__(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def deriv(self, x):\n",
    "        return (x > 0).astype(np.float32)\n",
    "\n",
    "\n",
    "class LeakyReLU:\n",
    "    def __call__(self, x):\n",
    "        ret = x\n",
    "        ret[ret < 0] *= 0.01\n",
    "        return ret.astype(np.float32)\n",
    "\n",
    "    def deriv(self, x):\n",
    "        ret = x\n",
    "        ret[ret < 0] = -0.01\n",
    "        ret[ret > 0] = 1.\n",
    "        return ret.astype(np.float32)\n",
    "\n",
    "\n",
    "class MSE:\n",
    "    def __call__(self, res, y):\n",
    "        return sum((i - j) * (i - j) for i, j in zip(res, y))\n",
    "\n",
    "    def deriv(self, res, y, z, activation_f):\n",
    "        return (res - y) * activation_f.deriv(z)\n",
    "\n",
    "\n",
    "class CrossEntropy:\n",
    "    def __call__(self, res, y):\n",
    "        return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n",
    "\n",
    "    def deriv(self, res, y, z, activation_f):\n",
    "        return res - y\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, shape, activation_f=ReLU(), cost_f=CrossEntropy()):\n",
    "        np.random.seed(2048)\n",
    "        self.shape = shape\n",
    "        self.w = [np.random.uniform(-(6 / x) ** 0.5, (6 / x) ** 0.5, x * y).reshape(y, x) for x, y in\n",
    "                  zip(shape[:-1], shape[1:])]\n",
    "        self.b = [np.random.uniform(-(6 / x) ** 0.5, (6 / x) ** 0.5, 1 * y).reshape(y, 1) for x, y in\n",
    "                  zip(shape[:-1], shape[1:])]\n",
    "        self.activation_f = activation_f\n",
    "        self.cost_f = cost_f\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = x.reshape(-1, 1)\n",
    "        for w, b in zip(self.w, self.b):\n",
    "            ret = self.activation_f(np.dot(w, ret) + b)\n",
    "        return ret.reshape(-1)\n",
    "\n",
    "    def backward(self, _x, _y):\n",
    "        dw = [np.zeros(w.shape) for w in self.w]\n",
    "        db = [np.zeros(b.shape) for b in self.b]\n",
    "        x, y = _x.reshape(-1, 1), _y.reshape(-1, 1)\n",
    "        a, z = [x], []\n",
    "\n",
    "        for w, b in zip(self.w, self.b):\n",
    "            x = np.dot(w, x) + b\n",
    "            z.append(x)\n",
    "            x = self.activation_f(x)\n",
    "            a.append(x)\n",
    "\n",
    "        dz = self.cost_f.deriv(a[-1], y, z[-1], self.activation_f)\n",
    "        dw[-1] = np.dot(dz, a[-2].transpose())\n",
    "        db[-1] = dz\n",
    "\n",
    "        for i in range(2, len(self.shape)):\n",
    "            dz = np.dot(self.w[-(i - 1)].transpose(), dz) * self.activation_f.deriv(z[-i])\n",
    "            dw[-i] = np.dot(dz, a[-(i + 1)].transpose())\n",
    "            db[-i] = dz\n",
    "\n",
    "        return (dw, db)\n",
    "\n",
    "    def update(self, batch, lr, lmbda, n):\n",
    "        dw = [np.zeros(w.shape) for w in self.w]\n",
    "        db = [np.zeros(b.shape) for b in self.b]\n",
    "\n",
    "        for x, y in batch:\n",
    "            _dw, _db = self.backward(x, y)\n",
    "            dw = [w + _w for w, _w in zip(dw, _dw)]\n",
    "            db = [b + _b for b, _b in zip(db, _db)]\n",
    "\n",
    "        self.w = [(1 - lr * (lmbda / n)) * w - (lr / len(batch)) * _w for w, _w in zip(self.w, dw)]\n",
    "        self.b = [b - (lr / len(batch)) * _b for b, _b in zip(self.b, db)]\n",
    "\n",
    "    def SGD(self, epochs, batch_size, lr, lmbda, train_data, data_per_epoch, test_data=None):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            np.random.shuffle(train_data)\n",
    "            batchs = [train_data[i:i + batch_size] for i in range(0, data_per_epoch, batch_size)]\n",
    "            for batch in batchs: self.update(batch, lr, lmbda, len(train_data))\n",
    "            if test_data:\n",
    "                print(f\"Epoch : {epoch}, Evaluate : {self.evaluate(test_data)} / {len(test_data)}\")\n",
    "            else:\n",
    "                print(f\"Epoch : {epoch}\")\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        ret = sum(int(np.argmax(self.forward(x)) == np.argmax(y)) for x, y in test_data)\n",
    "        return ret\n",
    "\n",
    "    def Calc(self, x):\n",
    "        y = self.forward(x)\n",
    "        return np.argmax(y)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lU_39vLIS4Iw",
    "outputId": "59d024c6-53dc-4531-81f7-1a06ef7e50a3"
   },
   "source": [
    "def __DataAugmentation(x, d):\n",
    "    dx = [0, -1, 0, 1]\n",
    "    dy = [1, 0, -1, 0]\n",
    "    x = x.reshape(28, 28)\n",
    "    ret = np.zeros(x.shape)\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            nx = i + dx[d]\n",
    "            ny = j + dy[d]\n",
    "            if nx < 0 or nx >= 28 or ny < 0 or ny >= 28: continue\n",
    "            ret[nx][ny] = x[i][j]\n",
    "    return ret.reshape(-1)\n",
    "\n",
    "def DataAugmentation(data):\n",
    "    n = len(data)\n",
    "    for i in range(n):\n",
    "        for j in range(4):\n",
    "            data.append((\n",
    "                __DataAugmentation(data[i][0], j), data[i][1]\n",
    "            ))\n",
    "    return data\n",
    "\n",
    "train_data = DataAugmentation(train_data)\n",
    "print(len(train_data))\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJJfnmZIPCBh",
    "outputId": "07031300-199d-4464-9fe9-d09c97b3b563"
   },
   "source": [
    "N = Network([784, 28, 28, 10])\n",
    "N.SGD(30, 10, 0.03, 0.1, train_data, 50000, test_data = test_data)\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Evaluate : 9342 / 10000\n",
      "Epoch : 2, Evaluate : 9473 / 10000\n",
      "Epoch : 3, Evaluate : 9507 / 10000\n",
      "Epoch : 4, Evaluate : 9575 / 10000\n",
      "Epoch : 5, Evaluate : 9588 / 10000\n",
      "Epoch : 6, Evaluate : 9609 / 10000\n",
      "Epoch : 7, Evaluate : 9625 / 10000\n",
      "Epoch : 8, Evaluate : 9633 / 10000\n",
      "Epoch : 9, Evaluate : 9637 / 10000\n",
      "Epoch : 10, Evaluate : 9640 / 10000\n",
      "Epoch : 11, Evaluate : 9661 / 10000\n",
      "Epoch : 12, Evaluate : 9673 / 10000\n",
      "Epoch : 13, Evaluate : 9674 / 10000\n",
      "Epoch : 14, Evaluate : 9698 / 10000\n",
      "Epoch : 15, Evaluate : 9663 / 10000\n",
      "Epoch : 16, Evaluate : 9699 / 10000\n",
      "Epoch : 17, Evaluate : 9690 / 10000\n",
      "Epoch : 18, Evaluate : 9684 / 10000\n",
      "Epoch : 19, Evaluate : 9678 / 10000\n",
      "Epoch : 20, Evaluate : 9707 / 10000\n",
      "Epoch : 21, Evaluate : 9689 / 10000\n",
      "Epoch : 22, Evaluate : 9708 / 10000\n",
      "Epoch : 23, Evaluate : 9693 / 10000\n",
      "Epoch : 24, Evaluate : 9688 / 10000\n",
      "Epoch : 25, Evaluate : 9717 / 10000\n",
      "Epoch : 26, Evaluate : 9690 / 10000\n",
      "Epoch : 27, Evaluate : 9701 / 10000\n",
      "Epoch : 28, Evaluate : 9698 / 10000\n",
      "Epoch : 29, Evaluate : 9707 / 10000\n",
      "Epoch : 30, Evaluate : 9706 / 10000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rW4qvpSFkd5_",
    "outputId": "f51b8f06-4251-482a-fbc2-ec38fff05adc"
   },
   "source": [
    "N.SGD(15, 10, 0.01, 0.05, train_data, 50000, test_data = test_data)"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch : 1, Evaluate : 9750 / 10000\n",
      "Epoch : 2, Evaluate : 9751 / 10000\n",
      "Epoch : 3, Evaluate : 9750 / 10000\n",
      "Epoch : 4, Evaluate : 9743 / 10000\n",
      "Epoch : 5, Evaluate : 9752 / 10000\n",
      "Epoch : 6, Evaluate : 9741 / 10000\n",
      "Epoch : 7, Evaluate : 9751 / 10000\n",
      "Epoch : 8, Evaluate : 9755 / 10000\n",
      "Epoch : 9, Evaluate : 9754 / 10000\n",
      "Epoch : 10, Evaluate : 9748 / 10000\n",
      "Epoch : 11, Evaluate : 9765 / 10000\n",
      "Epoch : 12, Evaluate : 9754 / 10000\n",
      "Epoch : 13, Evaluate : 9742 / 10000\n",
      "Epoch : 14, Evaluate : 9744 / 10000\n",
      "Epoch : 15, Evaluate : 9752 / 10000\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}